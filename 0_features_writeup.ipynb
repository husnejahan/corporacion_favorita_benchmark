{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grocery Favorita: AWS DeepAR vs Custom Seq2Seq\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0f/Corporaci%C3%B3n_Favorita_Logo.png\"> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Overview\n",
    "This notebook covers creating features (tensors and embeddings) for the Seq2Seq model. Only the top 5% most popular store-item pairs are considered for predictions; popularity is defined by the number of unique days sales took place. Additionally, the only features created are those that are readily available in the data provided; this is to benchmark the performance of a custom model versus the performance the deepAR timeseries model that AWS provides. Features are either instance specific (sales, promotion occurring, item details, store details) or shared among instances across dates (datetime details, oil price)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as dt\n",
    "import dask.dataframe as ddf\n",
    "from embedder.preprocessing import (categorize, pick_emb_dim, encode_categorical)\n",
    "from embedder.regression import Embedder\n",
    "from embedder.assessment import visualize\n",
    "import feather\n",
    "import plotly.offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# creates datetime featues when datetime column is given\n",
    "def generate_datetimedf(df_datetime_col):\n",
    "    \n",
    "    from workalendar.usa import California\n",
    "    from dateutil import easter\n",
    "    import calendar\n",
    "    import datetime as dt\n",
    "\n",
    "    first_date = min(df_datetime_col)\n",
    "    last_date = max(df_datetime_col)\n",
    "    all_years = (df_datetime_col.apply(lambda x: x.year)).unique()\n",
    "\n",
    "    cal = California()\n",
    "    all_holidays = []\n",
    "    for i in all_years:\n",
    "        holidays = [x[0] for x in cal.holidays(i)]\n",
    "        holidays = holidays+[easter.easter(i)]\n",
    "        all_holidays.append(holidays)\n",
    "\n",
    "    all_holidays = set([y for x in all_holidays for y in x])\n",
    "\n",
    "    weekends = set(['Saturday', 'Sunday'])\n",
    "    datetimedf = pd.DataFrame({\n",
    "#         'datetime_calendar_year': df_datetime_col.apply(lambda x: x.year),\n",
    "#                                'datetime_calendar_month': df_datetime_col.apply(lambda x: x.month),\n",
    "#                                'datetime_calendar_day': df_datetime_col.apply(lambda x: x.day),\n",
    "#                                'datetime_calendar_hour': df_datetime_col.apply(lambda x: x.hour),\n",
    "#                                'datetime_day_of_year': df_datetime_col.apply(lambda x: x.timetuple().tm_yday),\n",
    "                               'datetime_day_of_week': df_datetime_col.apply(lambda x: x.timetuple().tm_wday),\n",
    "#                                'datetime_day_of_cycle': df_datetime_col.apply(lambda x: (x-first_date).days+1),\n",
    "                               'datetime_is_weekend': df_datetime_col.apply(lambda x: 1 if calendar.day_name[x.weekday()] in weekends else 0),\n",
    "#                                'datetime_is_holiday': df_datetime_col.apply(lambda x: 1 if x.date() in all_holidays else 0)\n",
    "                               })\n",
    "\n",
    "#     holiday_1d = [x+dt.timedelta(days=1) for x in list(all_holidays)] + \\\n",
    "#         [x+dt.timedelta(days=-1) for x in list(all_holidays)]\n",
    "#     holiday_2d = holiday_1d+[x+dt.timedelta(days=2) for x in list(all_holidays)]+[\n",
    "#         x+dt.timedelta(days=-2) for x in list(all_holidays)]\n",
    "#     holiday_3d = holiday_2d+[x+dt.timedelta(days=3) for x in list(all_holidays)]+[\n",
    "#         x+dt.timedelta(days=-3) for x in list(all_holidays)]\n",
    "#     datetimedf['datetime_1d_away_holiday'] = df_datetime_col.apply(\n",
    "#         lambda x: 1 if x.date() in set(holiday_1d) else 0)\n",
    "#     datetimedf['datetime_2d_away_holiday'] = df_datetime_col.apply(\n",
    "#         lambda x: 1 if x.date() in set(holiday_2d) else 0)\n",
    "#     datetimedf['datetime_3d_away_holiday'] = df_datetime_col.apply(\n",
    "#         lambda x: 1 if x.date() in set(holiday_3d) else 0)\n",
    "#     holiday_7d = holiday_3d\n",
    "\n",
    "#     for d in [4, 5, 6, 7]:\n",
    "#         for i in [x+dt.timedelta(days=d) for x in list(all_holidays)]:\n",
    "#             holiday_7d.append(i)\n",
    "#         for t in [x+dt.timedelta(days=-d) for x in list(all_holidays)]:\n",
    "#             holiday_7d.append(t)\n",
    "\n",
    "#     datetimedf['datetime_within_7d_holiday'] = df_datetime_col.apply(\n",
    "#         lambda x: 1 if x.date() in set(holiday_7d) else 0)\n",
    "    \n",
    "    datetimedf.index = df_datetime_col\n",
    "\n",
    "    return datetimedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def read_csv_generator(path,preview_csv=False):\n",
    "    csvs = [x for x in os.listdir(path) if 'csv' in x]\n",
    "    \n",
    "    if not preview_csv:\n",
    "        for csv in csvs:\n",
    "            print(\n",
    "                \"%s_df = pd.read_csv('%s/%s')\" % (csv.replace('.csv',\n",
    "                                                              ''), path, csv)\n",
    "            )\n",
    "    else:\n",
    "        for csv in csvs:\n",
    "            print(csv)\n",
    "            display(pd.read_csv('%s/%s' % (path, csv) ).head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first time loading data\n",
    "# types_dict = {'id': 'int32',\n",
    "#              'item_nbr': 'int32',\n",
    "#              'store_nbr': 'int8',\n",
    "#              'unit_sales': 'float32'}\n",
    "\n",
    "# train_df = pd.read_csv('data/train.csv', low_memory=True, dtype=types_dict, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second time loading data\n",
    "train_df = feather.read_dataframe('data/train_feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions_df = pd.read_csv('data/transactions.csv')\n",
      "items_df = pd.read_csv('data/items.csv')\n",
      "oil_df = pd.read_csv('data/oil.csv')\n",
      "holidays_events_df = pd.read_csv('data/holidays_events.csv')\n",
      "stores_df = pd.read_csv('data/stores.csv')\n"
     ]
    }
   ],
   "source": [
    "read_csv_generator('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('data/transactions.csv')\n",
    "items_df = pd.read_csv('data/items.csv')\n",
    "oil_df = pd.read_csv('data/oil.csv')\n",
    "holidays_events_df = pd.read_csv('data/holidays_events.csv')\n",
    "stores_df = pd.read_csv('data/stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>2358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>3487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>4</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store_nbr  transactions\n",
       "0  2013-01-01         25           770\n",
       "1  2013-01-02          1          2111\n",
       "2  2013-01-02          2          2358\n",
       "3  2013-01-02          3          3487\n",
       "4  2013-01-02          4          1922"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>class</th>\n",
       "      <th>perishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96995</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99197</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103501</td>\n",
       "      <td>CLEANING</td>\n",
       "      <td>3008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103520</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103665</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_nbr        family  class  perishable\n",
       "0     96995     GROCERY I   1093           0\n",
       "1     99197     GROCERY I   1067           0\n",
       "2    103501      CLEANING   3008           0\n",
       "3    103520     GROCERY I   1028           0\n",
       "4    103665  BREAD/BAKERY   2712           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  dcoilwtico\n",
       "0  2013-01-01         NaN\n",
       "1  2013-01-02       93.14\n",
       "2  2013-01-03       92.97\n",
       "3  2013-01-04       93.12\n",
       "4  2013-01-07       93.20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holidays_events.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>locale</th>\n",
       "      <th>locale_name</th>\n",
       "      <th>description</th>\n",
       "      <th>transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Manta</td>\n",
       "      <td>Fundacion de Manta</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Regional</td>\n",
       "      <td>Cotopaxi</td>\n",
       "      <td>Provincializacion de Cotopaxi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-12</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Cuenca</td>\n",
       "      <td>Fundacion de Cuenca</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-14</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Libertad</td>\n",
       "      <td>Cantonizacion de Libertad</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-04-21</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Riobamba</td>\n",
       "      <td>Cantonizacion de Riobamba</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     type    locale locale_name                    description  \\\n",
       "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
       "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
       "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
       "3  2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
       "4  2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
       "\n",
       "   transferred  \n",
       "0        False  \n",
       "1        False  \n",
       "2        False  \n",
       "3        False  \n",
       "4        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr           city                           state type  cluster\n",
       "0          1          Quito                       Pichincha    D       13\n",
       "1          2          Quito                       Pichincha    D       13\n",
       "2          3          Quito                       Pichincha    D        8\n",
       "3          4          Quito                       Pichincha    D        9\n",
       "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "read_csv_generator('data',preview_csv=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to the Most Popular Store / Item Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_store_items = train_df.groupby(\n",
    "    ['store_nbr', 'item_nbr'])['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    174685.000000\n",
       "mean        718.419097\n",
       "std         514.831287\n",
       "min           1.000000\n",
       "25%         245.000000\n",
       "50%         651.000000\n",
       "75%        1133.000000\n",
       "max        1679.000000\n",
       "Name: date, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular_store_items.sort_values(ascending=False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_store_items_df = most_popular_store_items[most_popular_store_items > np.percentile(most_popular_store_items, 95)].sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "store_nbrs = set(list(most_popular_store_items_df['store_nbr']))\n",
    "item_nbrs = set(list(most_popular_store_items_df['item_nbr']))\n",
    "\n",
    "train_df = train_df[train_df.store_nbr.isin(store_nbrs)]\n",
    "train_df = train_df[train_df.item_nbr.isin(item_nbrs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Specific Features\n",
    "- Sales\n",
    "- Promotion\n",
    "- Item Details (Tile By Number of Dates)\n",
    "- Store Details (Tile By Number of Dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 46s, sys: 4min 25s, total: 16min 11s\n",
      "Wall time: 16min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "most_popular_store_items_df['raw_data'] = most_popular_store_items_df.apply(lambda row: train_df[(train_df.store_nbr == row['store_nbr']) & (train_df.item_nbr == row['item_nbr'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2013-03-16 00:00:00'), Timestamp('2017-07-04 00:00:00'))\n"
     ]
    }
   ],
   "source": [
    "# only create features between the latest earliest date and the earliest latest date for all instances\n",
    "recent_min_date = most_popular_store_items_df['raw_data'].apply(lambda x: x['date'].min()).max()\n",
    "latest_max_date = most_popular_store_items_df['raw_data'].apply(lambda x: x['date'].max()).min()\n",
    "\n",
    "print(recent_min_date, latest_max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_range_df=pd.DataFrame({'date':  pd.date_range(recent_min_date,latest_max_date,freq='D') })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "- at the store and item detail level\n",
    "- not done at the datetime level unless needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding lookups\n",
    "\n",
    "emb_store_lookup = stores_df[stores_df.store_nbr.isin(store_nbrs)].set_index('store_nbr').astype(str)\n",
    "emb_item_lookup = items_df[items_df.item_nbr.isin(item_nbrs)].set_index('item_nbr').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# must create embeddings using the latest date of each category: of these choose the earliest date\n",
    "min_max_emb_date = min(list(train_df.groupby(['store_nbr'])['date'].max().values) + list(train_df.groupby(['item_nbr'])['date'].max().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 3 µs, total: 7 µs\n",
      "Wall time: 11.2 µs\n",
      "Train on 1215573 samples, validate on 303894 samples\n",
      "Epoch 1/100\n",
      "1215573/1215573 [==============================] - 58s 47us/step - loss: 0.3011 - r2: 0.5929 - val_loss: 0.2702 - val_r2: 0.5631\n",
      "Epoch 2/100\n",
      "1215573/1215573 [==============================] - 59s 48us/step - loss: 0.2526 - r2: 0.6584 - val_loss: 0.2774 - val_r2: 0.5534\n",
      "Epoch 3/100\n",
      "1215573/1215573 [==============================] - 57s 47us/step - loss: 0.2413 - r2: 0.6737 - val_loss: 0.2551 - val_r2: 0.5873\n",
      "Epoch 4/100\n",
      "1215573/1215573 [==============================] - 60s 49us/step - loss: 0.2351 - r2: 0.6820 - val_loss: 0.2498 - val_r2: 0.5957\n",
      "Epoch 5/100\n",
      "1215573/1215573 [==============================] - 61s 50us/step - loss: 0.2314 - r2: 0.6871 - val_loss: 0.2474 - val_r2: 0.5999\n",
      "Epoch 6/100\n",
      "1215573/1215573 [==============================] - 62s 51us/step - loss: 0.2282 - r2: 0.6913 - val_loss: 0.2476 - val_r2: 0.5987\n",
      "Epoch 7/100\n",
      "1215573/1215573 [==============================] - 61s 50us/step - loss: 0.2258 - r2: 0.6945 - val_loss: 0.2478 - val_r2: 0.5995\n",
      "Epoch 8/100\n",
      "1215573/1215573 [==============================] - 61s 50us/step - loss: 0.2239 - r2: 0.6971 - val_loss: 0.2441 - val_r2: 0.6047\n",
      "Epoch 9/100\n",
      "1215573/1215573 [==============================] - 59s 49us/step - loss: 0.2220 - r2: 0.6997 - val_loss: 0.2422 - val_r2: 0.6077\n",
      "Epoch 10/100\n",
      "1215573/1215573 [==============================] - 59s 49us/step - loss: 0.2203 - r2: 0.7020 - val_loss: 0.2431 - val_r2: 0.6063\n",
      "Epoch 11/100\n",
      "1215573/1215573 [==============================] - 59s 48us/step - loss: 0.2188 - r2: 0.7041 - val_loss: 0.2416 - val_r2: 0.6087\n",
      "Epoch 12/100\n",
      "1215573/1215573 [==============================] - 59s 49us/step - loss: 0.2175 - r2: 0.7057 - val_loss: 0.2468 - val_r2: 0.6004\n",
      "Epoch 13/100\n",
      "1215573/1215573 [==============================] - 59s 48us/step - loss: 0.2163 - r2: 0.7074 - val_loss: 0.2462 - val_r2: 0.6012\n",
      "Epoch 14/100\n",
      "1215573/1215573 [==============================] - 59s 49us/step - loss: 0.2150 - r2: 0.7093 - val_loss: 0.2447 - val_r2: 0.6037\n",
      "Epoch 15/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.2138 - r2: 0.7108 - val_loss: 0.2492 - val_r2: 0.5965\n",
      "Epoch 16/100\n",
      "1215573/1215573 [==============================] - 63s 52us/step - loss: 0.2129 - r2: 0.7119 - val_loss: 0.2452 - val_r2: 0.6023\n",
      "Epoch 17/100\n",
      "1215573/1215573 [==============================] - 65s 54us/step - loss: 0.2118 - r2: 0.7135 - val_loss: 0.2464 - val_r2: 0.6012\n",
      "Epoch 18/100\n",
      "1215573/1215573 [==============================] - 59s 49us/step - loss: 0.2109 - r2: 0.7146 - val_loss: 0.2457 - val_r2: 0.6023\n",
      "Epoch 19/100\n",
      "1215573/1215573 [==============================] - 66s 55us/step - loss: 0.2100 - r2: 0.7159 - val_loss: 0.2464 - val_r2: 0.6011\n",
      "Epoch 20/100\n",
      "1215573/1215573 [==============================] - 70s 58us/step - loss: 0.2091 - r2: 0.7170 - val_loss: 0.2466 - val_r2: 0.6006\n",
      "Epoch 21/100\n",
      "1215573/1215573 [==============================] - 74s 61us/step - loss: 0.2083 - r2: 0.7181 - val_loss: 0.2477 - val_r2: 0.5986\n",
      "Epoch 22/100\n",
      "1215573/1215573 [==============================] - 60s 49us/step - loss: 0.2075 - r2: 0.7192 - val_loss: 0.2458 - val_r2: 0.6018\n",
      "Epoch 23/100\n",
      "1215573/1215573 [==============================] - 63s 52us/step - loss: 0.2068 - r2: 0.7201 - val_loss: 0.2468 - val_r2: 0.5998\n",
      "Epoch 24/100\n",
      "1215573/1215573 [==============================] - 59s 49us/step - loss: 0.2062 - r2: 0.7211 - val_loss: 0.2477 - val_r2: 0.5987\n",
      "Epoch 25/100\n",
      "1215573/1215573 [==============================] - 59s 49us/step - loss: 0.2053 - r2: 0.7223 - val_loss: 0.2502 - val_r2: 0.5948\n",
      "Epoch 26/100\n",
      "1215573/1215573 [==============================] - 62s 51us/step - loss: 0.2048 - r2: 0.7229 - val_loss: 0.2490 - val_r2: 0.5966\n",
      "Epoch 27/100\n",
      "1215573/1215573 [==============================] - 69s 57us/step - loss: 0.2042 - r2: 0.7236 - val_loss: 0.2538 - val_r2: 0.5892\n",
      "Epoch 28/100\n",
      "1215573/1215573 [==============================] - 74s 61us/step - loss: 0.2034 - r2: 0.7247 - val_loss: 0.2482 - val_r2: 0.5978\n",
      "Epoch 29/100\n",
      "1215573/1215573 [==============================] - 79s 65us/step - loss: 0.2030 - r2: 0.7254 - val_loss: 0.2528 - val_r2: 0.5908\n",
      "Epoch 30/100\n",
      "1215573/1215573 [==============================] - 69s 57us/step - loss: 0.2026 - r2: 0.7260 - val_loss: 0.2521 - val_r2: 0.5915\n",
      "Epoch 31/100\n",
      "1215573/1215573 [==============================] - 73s 60us/step - loss: 0.2019 - r2: 0.7268 - val_loss: 0.2503 - val_r2: 0.5939\n",
      "Epoch 32/100\n",
      "1215573/1215573 [==============================] - 101s 83us/step - loss: 0.2015 - r2: 0.7273 - val_loss: 0.2500 - val_r2: 0.5949\n",
      "Epoch 33/100\n",
      "1215573/1215573 [==============================] - 61s 50us/step - loss: 0.2010 - r2: 0.7280 - val_loss: 0.2496 - val_r2: 0.5951\n",
      "Epoch 34/100\n",
      "1215573/1215573 [==============================] - 70s 58us/step - loss: 0.2007 - r2: 0.7284 - val_loss: 0.2498 - val_r2: 0.5951\n",
      "Epoch 35/100\n",
      "1215573/1215573 [==============================] - 85s 70us/step - loss: 0.2002 - r2: 0.7291 - val_loss: 0.2504 - val_r2: 0.5942\n",
      "Epoch 36/100\n",
      "1215573/1215573 [==============================] - 81s 67us/step - loss: 0.1998 - r2: 0.7296 - val_loss: 0.2524 - val_r2: 0.5911\n",
      "Epoch 37/100\n",
      "1215573/1215573 [==============================] - 74s 61us/step - loss: 0.1994 - r2: 0.7302 - val_loss: 0.2530 - val_r2: 0.5902\n",
      "Epoch 38/100\n",
      "1215573/1215573 [==============================] - 61s 50us/step - loss: 0.1991 - r2: 0.7304 - val_loss: 0.2522 - val_r2: 0.5912\n",
      "Epoch 39/100\n",
      "1215573/1215573 [==============================] - 62s 51us/step - loss: 0.1987 - r2: 0.7311 - val_loss: 0.2532 - val_r2: 0.5896\n",
      "Epoch 40/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.1984 - r2: 0.7314 - val_loss: 0.2548 - val_r2: 0.5869\n",
      "Epoch 41/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.1981 - r2: 0.7319 - val_loss: 0.2536 - val_r2: 0.5886\n",
      "Epoch 42/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.1977 - r2: 0.7325 - val_loss: 0.2534 - val_r2: 0.5893\n",
      "Epoch 43/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.1974 - r2: 0.7329 - val_loss: 0.2565 - val_r2: 0.5844\n",
      "Epoch 44/100\n",
      "1215573/1215573 [==============================] - 64s 53us/step - loss: 0.1972 - r2: 0.7331 - val_loss: 0.2542 - val_r2: 0.5872\n",
      "Epoch 45/100\n",
      "1215573/1215573 [==============================] - 58s 47us/step - loss: 0.1969 - r2: 0.7337 - val_loss: 0.2539 - val_r2: 0.5883\n",
      "Epoch 46/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.1966 - r2: 0.7339 - val_loss: 0.2528 - val_r2: 0.5900\n",
      "Epoch 47/100\n",
      "1215573/1215573 [==============================] - 59s 49us/step - loss: 0.1963 - r2: 0.7343 - val_loss: 0.2517 - val_r2: 0.5920\n",
      "Epoch 48/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.1962 - r2: 0.7345 - val_loss: 0.2567 - val_r2: 0.5839\n",
      "Epoch 49/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.1958 - r2: 0.7351 - val_loss: 0.2556 - val_r2: 0.5853\n",
      "Epoch 50/100\n",
      "1215573/1215573 [==============================] - 58s 47us/step - loss: 0.1955 - r2: 0.7354 - val_loss: 0.2529 - val_r2: 0.5899\n",
      "Epoch 51/100\n",
      "1215573/1215573 [==============================] - 59s 48us/step - loss: 0.1954 - r2: 0.7355 - val_loss: 0.2537 - val_r2: 0.5884\n",
      "Epoch 52/100\n",
      "1215573/1215573 [==============================] - 58s 48us/step - loss: 0.1951 - r2: 0.7360 - val_loss: 0.2538 - val_r2: 0.5881\n",
      "Epoch 53/100\n",
      "1215573/1215573 [==============================] - 68s 56us/step - loss: 0.1949 - r2: 0.7361 - val_loss: 0.2560 - val_r2: 0.5846\n",
      "Epoch 54/100\n",
      "1215573/1215573 [==============================] - 63s 52us/step - loss: 0.1947 - r2: 0.7366 - val_loss: 0.2560 - val_r2: 0.5848\n",
      "Epoch 55/100\n",
      "1215573/1215573 [==============================] - 76s 62us/step - loss: 0.1946 - r2: 0.7367 - val_loss: 0.2579 - val_r2: 0.5817\n",
      "Epoch 56/100\n",
      "1215573/1215573 [==============================] - 91s 75us/step - loss: 0.1943 - r2: 0.7372 - val_loss: 0.2550 - val_r2: 0.5865\n",
      "Epoch 57/100\n",
      "1215573/1215573 [==============================] - 86s 70us/step - loss: 0.1942 - r2: 0.7372 - val_loss: 0.2548 - val_r2: 0.5870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "1215573/1215573 [==============================] - 88s 73us/step - loss: 0.1940 - r2: 0.7374 - val_loss: 0.2585 - val_r2: 0.5804\n",
      "Epoch 59/100\n",
      "1215573/1215573 [==============================] - 89s 73us/step - loss: 0.1938 - r2: 0.7378 - val_loss: 0.2586 - val_r2: 0.5805\n",
      "Epoch 60/100\n",
      "1215573/1215573 [==============================] - 88s 72us/step - loss: 0.1936 - r2: 0.7381 - val_loss: 0.2594 - val_r2: 0.5792\n",
      "Epoch 61/100\n",
      "1215573/1215573 [==============================] - 98s 80us/step - loss: 0.1933 - r2: 0.7382 - val_loss: 0.2581 - val_r2: 0.5816\n",
      "Epoch 62/100\n",
      "1215573/1215573 [==============================] - 105s 86us/step - loss: 0.1933 - r2: 0.7385 - val_loss: 0.2557 - val_r2: 0.5850\n",
      "Epoch 63/100\n",
      "1215573/1215573 [==============================] - 102s 84us/step - loss: 0.1931 - r2: 0.7387 - val_loss: 0.2593 - val_r2: 0.5799\n",
      "Epoch 64/100\n",
      "1215573/1215573 [==============================] - 102s 84us/step - loss: 0.1929 - r2: 0.7390 - val_loss: 0.2596 - val_r2: 0.5787\n",
      "Epoch 65/100\n",
      "1215573/1215573 [==============================] - 102s 84us/step - loss: 0.1927 - r2: 0.7392 - val_loss: 0.2644 - val_r2: 0.5716\n",
      "Epoch 66/100\n",
      "1215573/1215573 [==============================] - 103s 85us/step - loss: 0.1927 - r2: 0.7392 - val_loss: 0.2580 - val_r2: 0.5813\n",
      "Epoch 67/100\n",
      "1215573/1215573 [==============================] - 103s 84us/step - loss: 0.1926 - r2: 0.7394 - val_loss: 0.2613 - val_r2: 0.5759\n",
      "Epoch 68/100\n",
      "1215573/1215573 [==============================] - 103s 85us/step - loss: 0.1923 - r2: 0.7398 - val_loss: 0.2578 - val_r2: 0.5815\n",
      "Epoch 69/100\n",
      "1215573/1215573 [==============================] - 103s 85us/step - loss: 0.1923 - r2: 0.7398 - val_loss: 0.2579 - val_r2: 0.5813\n",
      "Epoch 70/100\n",
      "1215573/1215573 [==============================] - 104s 85us/step - loss: 0.1922 - r2: 0.7400 - val_loss: 0.2559 - val_r2: 0.5846\n",
      "Epoch 71/100\n",
      "1215573/1215573 [==============================] - 102s 84us/step - loss: 0.1920 - r2: 0.7402 - val_loss: 0.2585 - val_r2: 0.5802\n",
      "Epoch 72/100\n",
      "1215573/1215573 [==============================] - 101s 83us/step - loss: 0.1919 - r2: 0.7403 - val_loss: 0.2590 - val_r2: 0.5796\n",
      "Epoch 73/100\n",
      "1215573/1215573 [==============================] - 101s 83us/step - loss: 0.1918 - r2: 0.7405 - val_loss: 0.2614 - val_r2: 0.5760\n",
      "Epoch 74/100\n",
      "1215573/1215573 [==============================] - 99s 82us/step - loss: 0.1915 - r2: 0.7409 - val_loss: 0.2593 - val_r2: 0.5796\n",
      "Epoch 75/100\n",
      "1215573/1215573 [==============================] - 99s 82us/step - loss: 0.1915 - r2: 0.7408 - val_loss: 0.2576 - val_r2: 0.5821\n",
      "Epoch 76/100\n",
      "1215573/1215573 [==============================] - 99s 82us/step - loss: 0.1914 - r2: 0.7410 - val_loss: 0.2649 - val_r2: 0.5704\n",
      "Epoch 77/100\n",
      "1215573/1215573 [==============================] - 102s 84us/step - loss: 0.1913 - r2: 0.7409 - val_loss: 0.2609 - val_r2: 0.5767\n",
      "Epoch 78/100\n",
      "1215573/1215573 [==============================] - 98s 80us/step - loss: 0.1911 - r2: 0.7414 - val_loss: 0.2598 - val_r2: 0.5784\n",
      "Epoch 79/100\n",
      "1215573/1215573 [==============================] - 94s 78us/step - loss: 0.1911 - r2: 0.7413 - val_loss: 0.2581 - val_r2: 0.5811\n",
      "Epoch 80/100\n",
      "1215573/1215573 [==============================] - 95s 78us/step - loss: 0.1909 - r2: 0.7415 - val_loss: 0.2662 - val_r2: 0.5684\n",
      "Epoch 81/100\n",
      "1215573/1215573 [==============================] - 93s 76us/step - loss: 0.1909 - r2: 0.7417 - val_loss: 0.2606 - val_r2: 0.5769\n",
      "Epoch 82/100\n",
      "1215573/1215573 [==============================] - 94s 78us/step - loss: 0.1908 - r2: 0.7417 - val_loss: 0.2601 - val_r2: 0.5777\n",
      "Epoch 83/100\n",
      "1215573/1215573 [==============================] - 95s 79us/step - loss: 0.1907 - r2: 0.7420 - val_loss: 0.2587 - val_r2: 0.5802\n",
      "Epoch 84/100\n",
      "1215573/1215573 [==============================] - 84s 69us/step - loss: 0.1906 - r2: 0.7420 - val_loss: 0.2587 - val_r2: 0.5801\n",
      "Epoch 85/100\n",
      "1215573/1215573 [==============================] - 84s 69us/step - loss: 0.1905 - r2: 0.7421 - val_loss: 0.2604 - val_r2: 0.5773\n",
      "Epoch 86/100\n",
      "1215573/1215573 [==============================] - 84s 69us/step - loss: 0.1905 - r2: 0.7422 - val_loss: 0.2590 - val_r2: 0.5798\n",
      "Epoch 87/100\n",
      "1215573/1215573 [==============================] - 84s 69us/step - loss: 0.1903 - r2: 0.7425 - val_loss: 0.2624 - val_r2: 0.5747\n",
      "Epoch 88/100\n",
      "1215573/1215573 [==============================] - 84s 70us/step - loss: 0.1902 - r2: 0.7425 - val_loss: 0.2601 - val_r2: 0.5780\n",
      "Epoch 89/100\n",
      "1215573/1215573 [==============================] - 84s 69us/step - loss: 0.1901 - r2: 0.7426 - val_loss: 0.2591 - val_r2: 0.5793\n",
      "Epoch 90/100\n",
      "1215573/1215573 [==============================] - 85s 70us/step - loss: 0.1901 - r2: 0.7427 - val_loss: 0.2664 - val_r2: 0.5680\n",
      "Epoch 91/100\n",
      "1215573/1215573 [==============================] - 85s 70us/step - loss: 0.1900 - r2: 0.7428 - val_loss: 0.2668 - val_r2: 0.5675\n",
      "Epoch 92/100\n",
      "1215573/1215573 [==============================] - 86s 70us/step - loss: 0.1898 - r2: 0.7432 - val_loss: 0.2586 - val_r2: 0.5801\n",
      "Epoch 93/100\n",
      "1215573/1215573 [==============================] - 89s 73us/step - loss: 0.1898 - r2: 0.7433 - val_loss: 0.2634 - val_r2: 0.5722\n",
      "Epoch 94/100\n",
      "1215573/1215573 [==============================] - 84s 69us/step - loss: 0.1897 - r2: 0.7432 - val_loss: 0.2611 - val_r2: 0.5759\n",
      "Epoch 95/100\n",
      "1215573/1215573 [==============================] - 83s 69us/step - loss: 0.1897 - r2: 0.7433 - val_loss: 0.2609 - val_r2: 0.5762\n",
      "Epoch 96/100\n",
      "1215573/1215573 [==============================] - 83s 69us/step - loss: 0.1896 - r2: 0.7433 - val_loss: 0.2644 - val_r2: 0.5716\n",
      "Epoch 97/100\n",
      "1215573/1215573 [==============================] - 83s 68us/step - loss: 0.1895 - r2: 0.7434 - val_loss: 0.2624 - val_r2: 0.5744\n",
      "Epoch 98/100\n",
      "1215573/1215573 [==============================] - 83s 68us/step - loss: 0.1895 - r2: 0.7435 - val_loss: 0.2614 - val_r2: 0.5756\n",
      "Epoch 99/100\n",
      "1215573/1215573 [==============================] - 83s 68us/step - loss: 0.1894 - r2: 0.7437 - val_loss: 0.2577 - val_r2: 0.5822\n",
      "Epoch 100/100\n",
      "1215573/1215573 [==============================] - 83s 68us/step - loss: 0.1893 - r2: 0.7436 - val_loss: 0.2619 - val_r2: 0.5749\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def generate_embeddings():\n",
    "    # take only data points for the 2w for the training period for embeddings for timesake\n",
    "    X = train_df[(train_df['date'] >= (latest_max_date + dt.timedelta(-7)) + dt.timedelta(-56)) & (train_df['date'] <= (latest_max_date + dt.timedelta(-7)) )].merge(\n",
    "        generate_datetimedf(date_range_df['date']).applymap(str).reset_index(), on=['date'], how='left'\n",
    "    ).merge(\n",
    "        stores_df, on=['store_nbr'], how='left'\n",
    "    ).merge(\n",
    "        items_df, on=['item_nbr'], how= 'left'\n",
    "    )\n",
    "\n",
    "    emb_target = X['unit_sales'].apply(lambda x: 0 if x < 0 else x).apply(lambda x: np.log1p(x))\n",
    "\n",
    "    X = X.drop(columns=['id','date','unit_sales']).applymap(str)\n",
    "\n",
    "    cat_vars = categorize(X)\n",
    "    embedding_dict = pick_emb_dim(cat_vars, max_dim=50)\n",
    "    X_encoded, encoders = encode_categorical(X)\n",
    "\n",
    "    embedder = Embedder(embedding_dict)\n",
    "\n",
    "    embedder.fit(X_encoded, emb_target)\n",
    "    embeddings = embedder.get_embeddings()\n",
    "\n",
    "    return embeddings, encoders\n",
    "\n",
    "embeddings, encoders = generate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# extract embedding data\n",
    "def gernerate_all_embedding_lookup_df():\n",
    "    emb_key_df = pd.DataFrame(embeddings.keys(), columns=['key'])\n",
    "\n",
    "    def generate_embedding_df(col_):\n",
    "    #     if embeddings[col_].shape < 3:\n",
    "        col_emb_df = pd.DataFrame(\n",
    "            embeddings[col_],\n",
    "            index=encoders[col_].classes_,\n",
    "        ).rename_axis(col_).reset_index()\n",
    "    #     else:\n",
    "    \n",
    "        col_emb_df.columns = [col_] + [col_ + '_' + str(x) for x in list(col_emb_df) if x != col_]\n",
    "\n",
    "        return col_emb_df\n",
    "\n",
    "    emb_key_df['emb_data'] = emb_key_df['key'].apply(lambda x: generate_embedding_df(x))\n",
    "\n",
    "    return emb_key_df\n",
    "\n",
    "all_embedding_lookup_df = gernerate_all_embedding_lookup_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'onpromotion', u'onpromotion_0']\n",
      "['datetime_day_of_week', 'datetime_day_of_week_0', 'datetime_day_of_week_1', 'datetime_day_of_week_2', 'datetime_day_of_week_3']\n",
      "['datetime_is_weekend', 'datetime_is_weekend_0']\n"
     ]
    }
   ],
   "source": [
    "# merge embedding columns to .a master lookup\n",
    "def generate_master_instance_lookup():\n",
    "\n",
    "    master_instance_lookup = most_popular_store_items_df[['store_nbr','item_nbr']].merge(stores_df).merge(items_df).astype(str)\n",
    "\n",
    "    for df in all_embedding_lookup_df['emb_data']:\n",
    "        \n",
    "        try:\n",
    "            master_instance_lookup = master_instance_lookup.merge(df.astype(str))\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "            print list(df)\n",
    "            \n",
    "            \n",
    "    master_instance_lookup['store_nbr'] = master_instance_lookup['store_nbr'].astype(int)\n",
    "    master_instance_lookup['item_nbr'] = master_instance_lookup['item_nbr'].astype(int)\n",
    "\n",
    "    return master_instance_lookup\n",
    "\n",
    "master_instance_lookup = generate_master_instance_lookup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instance_specific_features(sales_df):\n",
    "\n",
    "    selected_item = sales_df['item_nbr'].values[0]\n",
    "    selected_store = sales_df['store_nbr'].values[0]\n",
    "\n",
    "    sales_df['unit_sales'] = sales_df['unit_sales'].apply(\n",
    "        lambda x: 0 if x < 0 else x)\n",
    "\n",
    "    # date range returns sales\n",
    "    sales_date_range_df = date_range_df.merge(sales_df[['date', 'unit_sales']], how='left').set_index(\n",
    "        'date').T.reset_index().rename(columns={'index': 'store_item'})\n",
    "    sales_date_range_df['store_item'] = 'store_%s_item_%s' % (\n",
    "        sales_df['store_nbr'].values[0], sales_df['item_nbr'].values[0])\n",
    "\n",
    "    # promotion tensor\n",
    "\n",
    "    promo_df = date_range_df.merge(\n",
    "        sales_df[['date', 'onpromotion']], how='left')\n",
    "    promo_df['onpromotion'] = promo_df['onpromotion'].fillna(\n",
    "        False).apply(lambda x: 1 if x else 0)\n",
    "\n",
    "    promo_tensor = promo_df['onpromotion'].values.reshape(\n",
    "        -1, 1).reshape(1, len(promo_df), 1)\n",
    "    \n",
    "    store_item_details_tensors = np.tile(\n",
    "    np.expand_dims(\n",
    "\n",
    "        master_instance_lookup.loc[\n",
    "            (master_instance_lookup['item_nbr'] == selected_item) & (\n",
    "                master_instance_lookup['store_nbr'] == selected_store),\n",
    "            [col for col in [col for col in list(master_instance_lookup) if '_' in col] if (\n",
    "                col != 'store_nbr') & (col != 'item_nbr')]\n",
    "        ].astype(float), axis=0\n",
    "\n",
    "    ),     (1, date_range_df.shape[0], 1))\n",
    "    \n",
    "    return sales_date_range_df, promo_tensor, store_item_details_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 49s, sys: 45.8 s, total: 5min 35s\n",
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = most_popular_store_items_df.apply(lambda row: create_instance_specific_features(row['raw_data']), axis=1)\n",
    "\n",
    "# %%time\n",
    "# df_dask_most_popular_store_items_df = ddf.from_pandas(most_popular_store_items_df, npartitions=10) \n",
    "# features = df_dask_most_popular_store_items_df.apply(lambda row: create_instance_specific_features(row['raw_data']), axis=1, meta=('object') ).compute(scheduler='multiprocessing')\n",
    "\n",
    "all_sales_df = pd.concat([i[0] for i in features]).reset_index(drop=True)\n",
    "promo_tensors = np.concatenate([i[1] for i in features], axis=0)\n",
    "store_item_tensors = np.concatenate([i[2] for i in features], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Datetime Tensors\n",
    "- Oil Price\n",
    "- Datetime Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# oil price\n",
    "\n",
    "oil_df['date']=pd.to_datetime(oil_df['date'])\n",
    "\n",
    "oil_df = date_range_df.merge(oil_df,how='left')\n",
    "\n",
    "oil_df['dcoilwtico']=oil_df['dcoilwtico'].interpolate()\n",
    "\n",
    "oil_df['dcoilwtico']=oil_df['dcoilwtico'].interpolate().fillna(method='bfill')\n",
    "\n",
    "oil_df['date'].describe()\n",
    "\n",
    "oil_values = (oil_df['dcoilwtico'].apply(lambda x: np.log1p(\n",
    "            x)) - oil_df['dcoilwtico'].apply(lambda x: np.log1p(x)).mean()).values\n",
    "\n",
    "oil_tensors = oil_values.reshape(-1, 1).reshape(1, len(oil_values), 1)\n",
    "\n",
    "oil_tensors = np.tile(oil_tensors, (most_popular_store_items_df.shape[0],1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# datetime tensors\n",
    "\n",
    "datetime_dummy_lookup = pd.get_dummies(generate_datetimedf(date_range_df['date']).applymap(str), drop_first=True).reset_index()\n",
    "# datetime_dummy_lookup.iloc[:,1:]\n",
    "dt_tensors = np.expand_dims(datetime_dummy_lookup.iloc[:,1:].values, axis=0)\n",
    "\n",
    "dt_tensors = np.tile(dt_tensors, (most_popular_store_items_df.shape[0],1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('exports/store_item_tensors',store_item_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('exports/promo_tensors',promo_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('exports/oil_tensors',oil_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('exports/dt_tensors',dt_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sales_df.to_pickle('exports/all_sales_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aws_deepAR_just_sales</td>\n",
       "      <td>8.627952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seq_just_sales</td>\n",
       "      <td>6.153596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seq_full</td>\n",
       "      <td>6.656257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model       mae\n",
       "0  aws_deepAR_just_sales  8.627952\n",
       "1         seq_just_sales  6.153596\n",
       "2               seq_full  6.656257"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'model': ['aws_deepAR_just_sales','seq_just_sales','seq_full'],\n",
    "    'mae': [8.627952, 6.153596, 6.656257]\n",
    "})[['model','mae']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom seq2seq model performs 28.67% better than AWS's deepAR algorithm in MAE. Other out-of-the-box features did not help the model's performance, meaning that improvements to the model could only be made by creating more complex features, such as lags across groupings, or parameter tuning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "180px",
    "width": "431px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
